"""BSHR loop implementation for research tasks.

This module implements a simplified version of the BSHR (Brainstorm,
Search, Hypothesise, Refine) loop.  It is inspired by the BSHR Loop
project【709386670875525†L254-L296】, which models human information
foraging behaviour.  The functions here provide lightweight hooks
for brainstorming search queries, performing searches against the
existing workflow system, formulating hypotheses, and refining those
hypotheses in subsequent iterations.

The BSHR loop is not tightly integrated into the scheduler by default.
Instead, it can be called on demand by a research agent or used in
ad‑hoc workflows where deeper investigation is required.  Each step
returns data structures that can be inspected or logged; the calling
code is responsible for iterating until a stopping condition is met.

Example usage::

    from bshr_loop import brainstorm, search, hypothesise, refine
    prompts = brainstorm("compare remote AI jobs in Europe")
    results = search(prompts)
    hypothesis = hypothesise(results)
    refined_prompts = refine(hypothesis, prompts)

Future improvements might include integrating external search APIs,
vector databases, or more sophisticated summarisation models.  For
now, this implementation uses the system's internal scraper and
summary functions to approximate the loop's behaviour.
"""

from __future__ import annotations

import logging
from typing import List, Dict

import config
import scraper
from agent import _openai_summary

logger = logging.getLogger(__name__)


def brainstorm(prompt: str, max_prompts: int = 5) -> List[str]:
    """Generate a list of search prompts based on an initial query.

    The brainstorming step expands the user's query into multiple
    targeted questions.  This naive implementation splits the prompt
    into individual words and combines them with preferred keywords
    defined in :mod:`config`.  Duplicate prompts are removed.

    Args:
        prompt: The user's original query or information need.
        max_prompts: Maximum number of prompts to generate.

    Returns:
        A list of unique search prompts.
    """
    if not prompt:
        return []
    words = [w.strip() for w in prompt.split() if w.strip()]
    prompts: List[str] = []
    # Combine prompt words with preferred keywords to broaden the search
    for word in words:
        for kw in config.PREFERRED_KEYWORDS:
            candidate = f"{word} {kw}"
            if candidate not in prompts:
                prompts.append(candidate)
            if len(prompts) >= max_prompts:
                break
        if len(prompts) >= max_prompts:
            break
    # Always include the original prompt as the first element
    prompts.insert(0, prompt)
    unique_prompts = []
    for p in prompts:
        if p not in unique_prompts:
            unique_prompts.append(p)
    return unique_prompts[:max_prompts]


def search(prompts: List[str]) -> List[Dict[str, str]]:
    """Search for information based on the brainstormed prompts.

    This function leverages the existing :mod:`scraper` module to
    retrieve job listings or other data relevant to the prompts.  It
    aggregates the results into a single list without deduplication.

    Args:
        prompts: A list of search queries generated by :func:`brainstorm`.

    Returns:
        A list of job dictionaries as returned by :func:`scraper.scrape_jobs`.
    """
    results: List[Dict[str, str]] = []
    for p in prompts:
        try:
            # In this simple implementation, we ignore the prompt
            # because the scraper already contains the search logic
            # across multiple sources.  Future versions could pass
            # the prompt to a search API or adapt the scraper to
            # accept a query parameter.
            jobs = scraper.scrape_jobs()
            if jobs:
                results.extend(jobs)
        except Exception as e:
            logger.error("BSHR search failed for prompt '%s': %s", p, e)
    return results


def hypothesise(results: List[Dict[str, str]]) -> str:
    """Formulate a hypothesis based on gathered search results.

    The hypothesis is generated by summarising the titles and
    companies of the results.  If the OpenAI API key is available,
    :func:`agent._openai_summary` is used; otherwise, a simple
    concatenation of job titles is returned.

    Args:
        results: A list of job dictionaries from :func:`search`.

    Returns:
        A string representing the current hypothesis or summary.
    """
    if not results:
        return "No results found to formulate a hypothesis."
    # Create a text prompt summarising job titles and companies
    descriptions = [f"{job.get('title')} at {job.get('company')}" for job in results[:10]]
    prompt = (
        "Summarise the following job opportunities and infer any common themes. "
        "Jobs: " + "; ".join(descriptions)
    )
    summary = _openai_summary(prompt)
    if summary:
        return summary
    # Fallback: simple list of job titles
    return "; ".join(descriptions)


def refine(hypothesis: str, previous_prompts: List[str], max_prompts: int = 5) -> List[str]:
    """Refine the search prompts based on the current hypothesis.

    This implementation appends the word "refine" and key nouns from the
    hypothesis to the previous prompts.  It aims to narrow or adjust
    the search focus for the next iteration.

    Args:
        hypothesis: The current hypothesis generated by :func:`hypothesise`.
        previous_prompts: Prompts used in the previous iteration.
        max_prompts: Maximum number of new prompts to return.

    Returns:
        A list of new prompts to be used in the next iteration of the
        BSHR loop.
    """
    if not hypothesis:
        return previous_prompts
    # Extract simple nouns from the hypothesis by splitting on spaces
    words = [w.strip().lower() for w in hypothesis.split() if len(w) > 4]
    new_prompts: List[str] = []
    for w in words:
        candidate = f"refine {w}"
        if candidate not in new_prompts and candidate not in previous_prompts:
            new_prompts.append(candidate)
        if len(new_prompts) >= max_prompts:
            break
    # Combine previous prompts with new ones, keeping uniqueness
    combined: List[str] = []
    for p in previous_prompts + new_prompts:
        if p not in combined:
            combined.append(p)
        if len(combined) >= max_prompts:
            break
    return combined